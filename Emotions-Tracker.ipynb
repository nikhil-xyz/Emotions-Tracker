{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef540ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cloning dataset from github\n",
    "# ! git clone https://github.com/nikhil-xyz/Text-classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f50d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Text-classification/go_emotions_dataset.csv', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56bb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f6713a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data, columns = ['example_very_unclear'])\n",
    "data.drop(['example_very_unclear_False'], axis=1, inplace=True)\n",
    "data = data.rename(columns = {'example_very_unclear_False':'example_very_unclear'})\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81529ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "text                         0\n",
       "admiration                   0\n",
       "amusement                    0\n",
       "anger                        0\n",
       "annoyance                    0\n",
       "approval                     0\n",
       "caring                       0\n",
       "confusion                    0\n",
       "curiosity                    0\n",
       "desire                       0\n",
       "disappointment               0\n",
       "disapproval                  0\n",
       "disgust                      0\n",
       "embarrassment                0\n",
       "excitement                   0\n",
       "fear                         0\n",
       "gratitude                    0\n",
       "grief                        0\n",
       "joy                          0\n",
       "love                         0\n",
       "nervousness                  0\n",
       "optimism                     0\n",
       "pride                        0\n",
       "realization                  0\n",
       "relief                       0\n",
       "remorse                      0\n",
       "sadness                      0\n",
       "surprise                     0\n",
       "neutral                      0\n",
       "example_very_unclear_True    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b593871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing panctuations\n",
    "import string\n",
    "data['text'] = data['text'].apply(lambda word:[letters.lower() for letters in word if letters not in string.punctuation])\n",
    "data['text'] = data['text'].apply(lambda word:''.join(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab30952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tarun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tarun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Tarun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7de2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8cf7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  text = text.lower()\n",
    "  text = text.split()\n",
    "  text = [wnl.lemmatize(word) for word in text if not word in stopwords.words('english')]\n",
    "  text = ' '.join(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bd08a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65bbec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sexuality shouldnâ€™t grouping category make different othet ppl imo fit definition grouping'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950b8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(data['text'])\n",
    "y = list(data[data.columns[2:]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb4bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a4d885e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "307e0167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.26.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tarun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef1f2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "938d9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16ddd9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c6cb2ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataset\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.FloatTensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_encodings, y_train)\n",
    "test_dataset = Dataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f4564",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[0]\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc25fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1cc287",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\", return_dict=True, num_labels=29)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset             # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c48297",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32226d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.predict(test_dataset)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d40313",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c87de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, output)\n",
    "# precision = precision_score(y_test, output)\n",
    "# recall = recall_score(y_test, output)\n",
    "# f1 = f1_score(y_test, output)\n",
    "\n",
    "print(\"accuracy: \", accuracy)\n",
    "# print(\"precision: \", precision)\n",
    "# print(\"Recall: \", recall)\n",
    "# print(\"F1-score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('assignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c67994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea55c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
